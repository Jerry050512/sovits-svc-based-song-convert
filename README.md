# Singing Voice Cloning with So-Vits SVC

This guide provides a comprehensive walkthrough for setting up and utilizing the So-Vits SVC model to clone singing voices. Follow these steps to prepare your data, train your custom model, and perform voice inference.

Here is a [‰∏≠ÊñáÁÆÄ‰Ωì](./README_zh_CN.md) README version at the repository as well.

## üöÄ Getting Started

The process generally involves preparing your audio data, training a custom model, and then using that model to predict and convert new vocal tracks.

### 1. Prepare Your Data

Before you can train your model, you need a collection of high-quality vocal recordings.

#### 1.1 Obtain Song Data

1.  **Download Songs:** Acquire songs from the internet.
2.  **Extract Vocals:** Separate the vocal track from the background music.
3.  **Split Audio:** Divide the extracted vocal audio into smaller segments (ideally 5-10 seconds each).
4.  **Train Model:** Use this processed vocal data to train your custom voice model.
5.  **Infer New Songs:** Extract vocals from a new song, convert them using your trained model, and then remix them with the original music.

For this demo, we've compiled **84 Mayday songs** from 3 albums, 1 music collection, 1 live album, and 5 singles released over the past 15 years. The total duration is approximately 6 hours, with valid vocal data amounting to around 5 hours and 19 minutes.

#### 1.2 Extract Vocal Voice from Music

If you don't have pure vocal tracks, you'll need to separate them from the full songs. Here are two recommended tools:

* **Ultimate Vocal Remover (UVR)**: Highly recommended for its effectiveness.
* **Spleeter**: A well-known Python library for splitting audio into various stems, including vocals and instrumental tracks.

***Skip this section if you already have pure vocal audio files.***



#### 1.3 Pre-Process Your Audio Data

Once you have your pure vocal tracks, you need to prepare them for training.

* **Split Audio Files:** Divide your vocal audio into segments of 5-10 seconds. You can use the `utils/audio_split.py` script for this.

    ```python
    from utils.audio_split import split_audio_files

    split_audio_files(
        input_directory="path/to/your/raw_vocal_audio",
        output_directory="path/to/your/split_audio_output",
        audio_format="wav",
        segment_duration_ms=5000, # 5000ms = 5 seconds
        naming_convention="hash"
    )
    ```

* **Remove Unnecessary Pieces (e.g., Silence):** Clean your dataset by removing silent or low-loudness segments using `utils/audio_clean.py`.

    ```python
    from utils.audio_clean import delete_low_loudness_audio_files

    # Set a loudness threshold. For example, -30 dBFS means anything quieter will be deleted.
    loudness_threshold = -30.0
    delete_low_loudness_audio_files("path/to/your/split_audio_output", loudness_threshold)
    ```



## ‚öîÔ∏è Strike for Training!

With your data prepared, it's time to set up and train the models.

### 2.1 Pre-Trained Model Download

Download the necessary pre-trained models and place them in the specified directories:

* **So-Vits:** [D_0.pth, G_0.pth](https://huggingface.co/langeheris/Sovits-4.0-V2-Pretrained-Model/tree/main)
* **DDSP-SVC:** [model_0.pt](https://github.com/yxlllc/DDSP-SVC/releases/tag/5.0)
* **NSF-HIFIGAN:** [nsf_hifigan_20221211.zip](https://github.com/openvpi/vocoders/releases/download/nsf-hifigan-v1/nsf_hifigan_20221211.zip)

**Placement Instructions:**

1.  Place `checkpoint_best_legacy_500.pt` into the `pretrain` directory.
2.  Place `D_0.pth` and `G_0.pth` into the `logs/44k` directory.
3.  Place `model_0.pt` into the `logs/44k/diffusion` directory.
4.  Extract the four files from `nsf_hifigan_20221211.zip` and place them into the `pretrain/nsf_hifigan` directory.



### 2.2 Prepare the Dataset for Training

Run these commands to process your audio files and generate the dataset required for training:

```bash
python resample.py
python preprocess_flist_config.py --speech_encoder vec768l12
python preprocess_hubert_f0.py --f0_predictor dio
python preprocess_hubert_f0.py --f0_predictor dio --use_diff --num_processes 4
```

These commands perform the following essential steps:

* **Resample audio files:** Ensures all audio is at a consistent sample rate.
* **Generate and split the dataset:** Prepares the data into a format suitable for model training.
* **Preprocess Hubert and F0:** Extracts features crucial for voice cloning.

**Note:** Adjust the `--num_processes` argument based on your system's CPU core count for optimal performance.



## üèãÔ∏è Train!

Now you're ready to train the main model and the diffusion model.

* **Train the Main Model:**

    ```bash
    python train.py -c configs/config.json -m 44k
    ```

* **Train the Diffusion Model:**

    ```bash
    python train_diff.py -c configs/diffusion.yaml
    ```



## üé§ Infer

After successful training, you can use your model to convert new vocal tracks.

**Example Inference Command:**

```bash
python inference_main.py -m "logs/44k/G_94400.pth" -c "configs/config.json" -n "lemon.wav" -t 0 -s "ashin" -lg 1 -shd -dm "logs/44k/diffusion/model_9000.pt"
```

**Command Line Arguments Explained:**

* `-m`: Path to the main generator model (`.pth` file).
* `-c`: Path to the configuration file (`.json`).
* `-n`: Name/path of the input audio file to be converted.
* `-t`: Transpose value (e.g., `0` for no transpose, positive/negative for higher/lower pitch).
* `-s`: Speaker name (if you have multiple speakers in your dataset).
* `-lg`: Output loudness gain.
* `-shd`: **Use Shallow Diffusion.** Include this flag to enable shallow diffusion for inference.
* `-dm`: **Diffusion model path.** Specifies the path to your trained diffusion model.